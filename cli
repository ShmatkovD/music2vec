#!/usr/bin/env python

import click
from preprocess_dataset.process_dataset import main as pds
from model.evaluate import evaluate
from model.train import main as tr
from model.demo import demonstrate


@click.group()
def cli():
    pass


@cli.command('preprocess_dataset', help='Preprocess dataset')
@click.option('-d', '--dataset-dir', required=True, help='Directory with dataset')
@click.option('-s', '--dataset-size', required=True, help='Size of processing dataset')
@click.option('-a', '--audio-processor', default='mfcc', help='Audio preprocessor')
@click.option('-o', '--output-dir', required=True, help='Output directory')
@click.option('-t', '--test-size', default=0.25, type=float, help='How much data will be used in test part of dataset')
@click.option('-v', '--validate-size', default=0.05, type=float, help='How much data will be used in validate part of dataset')
def preprocess_dataset(
        dataset_dir,
        dataset_size,
        audio_processor,
        output_dir,
        test_size,
        validate_size
):
    pds(
        dataset_dir,
        dataset_size,
        audio_processor,
        output_dir,
        test_size,
        validate_size,
    )


@cli.command('train', help='Train music2vec model')
@click.option('-d', '--dataset', help='Path to dataset')
@click.option('-o', '--output', help='Models output dir')
def train(dataset, output):
    tr(dataset, output)


@cli.command('evaluate', help='Evaluate music2vec model')
@click.option('-d', '--dataset', help='Path to dataset')
def train(dataset):
    evaluate(dataset)


@cli.command('demo', help='Demo of m2v classifier')
@click.option('-f', '--music-file', help='.mp3')
@click.option('-m', '--model-path')
@click.option('-g', '--genres-metadata-file')
def demo(music_file, model_path, genres_metadata_file):
    demonstrate(music_file, model_path, genres_metadata_file)


if __name__ == '__main__':
    cli()
